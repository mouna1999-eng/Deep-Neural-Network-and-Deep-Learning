{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_nhU8bnliWa"
   },
   "source": [
    "# <font color='black'>Deep Neural Networks</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufwLY7O1liWc"
   },
   "source": [
    "## Set-up\n",
    "\n",
    "Firstly you will import all the packages used through the notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19624,
     "status": "ok",
     "timestamp": 1665821784721,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "9ECukxe0o89m",
    "outputId": "38c80828-b6f3-4360-d164-a9fe9a8e33cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "                                                                                from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/0IPA/Ma512/TP/TP2/') # The location of the .ipynb file.  \n",
    "import lr_utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1665821788511,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "sCSkjeooliWc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcKrDQNAliWd"
   },
   "source": [
    "## Initialization\n",
    "\n",
    "Start by reading the following function. It allows to initialize the parameters of a deep neural network. The number of units in the different layers are passed as argument with `layer_dims`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1665821790924,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "ZGRl6pA3liWe"
   },
   "outputs": [],
   "source": [
    "def initialization(layer_dims):\n",
    "               \n",
    "    np.random.seed(4870)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1 # integer representing the number of layers\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        ### He's initialization.\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * np.sqrt(4./(layer_dims[l-1]+layer_dims[l]))\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ###\n",
    "\n",
    "    assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "    assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDpeSl2iliWe"
   },
   "source": [
    "## Forward propagation\n",
    "\n",
    "The forward propagation has been split in different steps. Firstly, the linear forward module computes the following equations:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "where $A^{[0]} = X$. \n",
    "\n",
    "Define a function to compute $Z^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1665821795640,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "kUTYeX4TliWf"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1665821797147,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "G1FNFv8FliWf",
    "outputId": "486f8cd2-8351-43dd-aa22-12561e5f157e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[-0.67356113  0.67062057]]\n",
      "linear_cache = (array([[ 1.78862847,  0.43650985],\n",
      "       [ 0.09649747, -1.8634927 ],\n",
      "       [-0.2773882 , -0.35475898]]), array([[-0.08274148, -0.62700068, -0.04381817]]), array([[-0.47721803]]))\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))\n",
    "print(\"linear_cache = \" + str(linear_cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAVxU6galiWg"
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> Z= </td>\n",
    "    <td> [[ -0.67356113 0.67062057]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL7tiZWDliWg"
   },
   "source": [
    "### Activation Fcuntions\n",
    "\n",
    "In the first notebook you implemented the sigmoid function:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
    "\n",
    "In this notebook, you will need to implement the ReLU activation defined as:\n",
    "\n",
    "- **ReLU**: $A = RELU(Z) = max(0, Z)$. \n",
    "\n",
    "Complete the function below that computes the ReLU an activation fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1665821801190,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "kAw1lTVqliWi"
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    ### END CODE HERE ###\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "\n",
    "    cache=Z\n",
    "    ### START CODE HERE ###\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    ### END CODE HERE ###\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vQOG3DiliWi"
   },
   "source": [
    "You have implemented a function that determines the linear foward step. You will now combine the output of this function with either a sigmoid() or a relu() activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1665821809307,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "0L__tkHTliWi"
   },
   "outputs": [],
   "source": [
    "def forward_one(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvMYkAanliWj"
   },
   "source": [
    "### Forward propagation model\n",
    "\n",
    "The structure you will implement in this exercise consists on $L-1$ layers using a ReLU activation function and a last layer using a sigmoid.\n",
    "Implement the forward propagation of the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1665821810762,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "23cLRnCOliWj"
   },
   "outputs": [],
   "source": [
    "def forward_all(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ###\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        ### END CODE HERE ###\n",
    "      \n",
    "        caches.append(cache)   #\n",
    "    ### START CODE HERE ###\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asHWo1_7liWk"
   },
   "source": [
    "###  Cost function\n",
    "\n",
    "You will now compute the cross-entropy cost $J$, for all the training set using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1665821815874,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "oADdUu3NliWk"
   },
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    XX = np.multiply(np.log(AL),Y) +  np.multiply(np.log(1-AL), (1-Y))\n",
    "    cost = (-1/m)*np.sum(XX)\n",
    "    ### END CODE HERE ###\n",
    "    cost = np.squeeze(cost)      #  Eliminates useless dimensionality for the variable cost.\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1665821817055,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "u-InQrkDliWk",
    "outputId": "f186f22f-7c38-45c8-f4f7-cc9a86a88f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost()\n",
    "print(\"cost = \" + str(cost_function(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiS-kOAdliWk"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 0.2797765635793422</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQLJFIkZliWl"
   },
   "source": [
    "##  Backpropagation \n",
    "\n",
    "You will now implement the functions that will help you compute the gradient of the loss function with respect to the different parameters.\n",
    "\n",
    "To move backward in the computational graph you need to apply the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6bFEYzOliWl"
   },
   "source": [
    "### Linear backward\n",
    "\n",
    "For each layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$. The formulas you saw in class are:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1665821821471,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "VULBSt56liWl"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ###  (â‰ˆ 3 lines of code)\n",
    "    dW = (1./m)*(np.dot(dZ, A_prev.T))\n",
    "    db = (1./m)*(np.sum(dZ, axis = 1, keepdims=True))\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1665821823295,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "iUF2WNgYliWl",
    "outputId": "1e22475f-dafe-484c-8c23-37a29c4bbd1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 1.62477986e-01  2.08119187e+00 -1.34890293e+00 -8.08822550e-01]\n",
      " [ 1.25651742e-02 -2.21287224e-01 -5.90636554e-01  4.05614891e-03]\n",
      " [ 1.98659671e-01  2.39946554e+00 -1.86852905e+00 -9.65910523e-01]\n",
      " [ 3.18813678e-01 -9.92645222e-01 -6.57125623e-01 -1.46564901e-01]\n",
      " [ 2.48593418e-01 -1.19723579e+00 -4.44132647e-01 -6.09748046e-04]]\n",
      "dW = [[-1.05705158 -0.98560069 -0.54049797  0.10982291  0.53086144]\n",
      " [ 0.71089562  1.01447326 -0.10518156  0.34944625 -0.12867032]\n",
      " [ 0.46569162  0.31842359  0.30629837 -0.01104559 -0.19524287]]\n",
      "db = [[ 0.5722591 ]\n",
      " [ 0.04780547]\n",
      " [-0.38497696]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baACu1v8liWl"
   },
   "source": [
    "** Expected Output**:\n",
    "    \n",
    "```\n",
    "dA_prev = \n",
    "[[  1.62477986e-01   2.08119187e+00  -1.34890293e+00  -8.08822550e-01]\n",
    " [  1.25651742e-02  -2.21287224e-01  -5.90636554e-01   4.05614891e-03]\n",
    " [  1.98659671e-01   2.39946554e+00  -1.86852905e+00  -9.65910523e-01]\n",
    " [  3.18813678e-01  -9.92645222e-01  -6.57125623e-01  -1.46564901e-01]\n",
    " [  2.48593418e-01  -1.19723579e+00  -4.44132647e-01  -6.09748046e-04]]\n",
    "dW = \n",
    "[[-1.05705158 -0.98560069 -0.54049797  0.10982291  0.53086144]\n",
    " [ 0.71089562  1.01447326 -0.10518156  0.34944625 -0.12867032]\n",
    " [ 0.46569162  0.31842359  0.30629837 -0.01104559 -0.19524287]]\n",
    "db = \n",
    "[[ 0.5722591 ]\n",
    " [ 0.04780547]\n",
    " [-0.38497696]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxSkNSVRliWm"
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "Now you need to write the code that computes the derivatives for the activation functions. You have learned the derivatives for the sigmoid and the ReLU during theory class.\n",
    "Complete the two function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1665821828392,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "fsXjSrwHliWm"
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):    \n",
    "    Z = cache\n",
    "    ### START CODE HERE ###  (â‰ˆ 2 lines of code)\n",
    "    k = 1/(1+np.exp(-Z))\n",
    "    dZ=dA*k*(k-1)    # we have : Z=W A+b\n",
    "    ### END CODE HERE ###\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    ### START CODE HERE ###  (â‰ˆ 1 line of code)\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    ### END CODE HERE ###\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCMxGWshliWm"
   },
   "source": [
    "### One backpropagation step\n",
    "\n",
    "Next, you will create a function that implements one step of backpropagation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1665821830004,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "ItSs4URXliWn"
   },
   "outputs": [],
   "source": [
    "def backward_one(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache  \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1665821834675,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "qlnmAzuDliWn",
    "outputId": "d640feea-91eb-4839-9da1-9c8e466d90d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[-0.00401564 -0.0404019 ]\n",
      " [ 0.01386864  0.13953419]\n",
      " [-0.00747737 -0.07523079]]\n",
      "dW = [[-0.03615272  0.09887085 -0.03247948]]\n",
      "db = [[-0.06684355]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.01679913  0.16610885]\n",
      " [-0.05801838 -0.57368247]\n",
      " [ 0.031281    0.30930474]]\n",
      "dW = [[ 0.14820532 -0.40668077  0.13325465]]\n",
      "db = [[0.27525652]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test()\n",
    "\n",
    "dA_prev, dW, db = backward_one(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward_one(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlt4MrPduKNw"
   },
   "source": [
    "**Expected**\n",
    "sigmoid:\n",
    "dA_prev = [[ 0.00401564  0.0404019 ]\n",
    " [-0.01386864 -0.13953419]\n",
    " [ 0.00747737  0.07523079]]\n",
    "dW = [[ 0.03615272 -0.09887085  0.03247948]]\n",
    "db = [[0.06684355]]\n",
    "\n",
    "relu:\n",
    "dA_prev = [[ 0.01679913  0.16610885]\n",
    " [-0.05801838 -0.57368247]\n",
    " [ 0.031281    0.30930474]]\n",
    "dW = [[ 0.14820532 -0.40668077  0.13325465]]\n",
    "db = [[0.27525652]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbG0Z5qqliWn"
   },
   "source": [
    "### Backpropagation model\n",
    "\n",
    "Now you will put all together to compute the backward function for the whole network. \n",
    "In the backpropagation step, you will use the variables you stored in cache in the `forward_all` function to compute the gradients. You will iterate from the last layer backwards to layer $1$.\n",
    "\n",
    "You need to start by computing the derivative of the loss function with respect to $A^{[L]}$. And propagate this gradient backward thourgh all the layers in the network.\n",
    "\n",
    "You need to save each dA, dW and db in the grads dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1665821839563,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "nHTXCAMXliWn"
   },
   "outputs": [],
   "source": [
    "def backward_all(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "\n",
    "    ### START CODE HERE ###  (â‰ˆ 1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_one(dAL, current_cache,activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_one(grads[\"dA\" + str(l + 1)],current_cache,activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1665821843557,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "zIAPPWxEliWn",
    "outputId": "59f9c2ae-f75b-47c0-8244-71ce987b4235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[-0.41010002 -0.07807203 -0.13798444 -0.10502167]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.05283652 -0.01005865 -0.01777766 -0.0135308 ]]\n",
      "db1 = [[0.22007063]\n",
      " [0.        ]\n",
      " [0.02835349]]\n",
      "dA1 = [[-0.12913162  0.44014127]\n",
      " [ 0.14175655 -0.48317296]\n",
      " [-0.01663708  0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = backward_all_test()\n",
    "grads = backward_all(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99n02BkKliWo"
   },
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr>  \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr>   \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xl9I8E2liWo"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Finally you can update the parameters of the model according: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1665821847961,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "gTdUmYeUliWo"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1665821849462,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "CtG21bCOliWo",
    "outputId": "9776bb9c-8d8c-4ce6-d583-44e5aae476d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = gradient_descent_test_case()\n",
    "parameters = gradient_descent(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgf9j60uliWo"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiPCdg3DliWp"
   },
   "source": [
    "You can now create a deep neural network  combining all the functions defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHECSjmaxMY8"
   },
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1665821858148,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "jAnXC1OepmcM"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "#    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_dataset = h5py.File(\"/content/gdrive/My Drive/0IPSA/Ma512/TP/TP2/datasets/train_catvnoncat.h5\", \"r\")  #You need to upload the dataset in the right folder.\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "#    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_dataset = h5py.File(\"/content/gdrive/My Drive/0IPSA/Ma512/TP/TP2/datasets/test_catvnoncat.h5\", \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y-93Xv0216N"
   },
   "source": [
    "#Exercise 1, Predictions on a dataset.\n",
    "Create a function that takes as argument the images, the labels and the parameters to create a prediction using the forward_all function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1665821891517,
     "user": {
      "displayName": "Mouna Jarai",
      "userId": "09150871145638206120"
     },
     "user_tz": -120
    },
    "id": "whI5mGcf3BmI"
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = forward_all(X, parameters)\n",
    "    # convert probas to 0/1 predictions, if probas [0,i] > 0.5-> p[0,i]=1 else p[0,i]=0\n",
    "    for i in range(0, probas.shape[1]):\n",
    "      ### START CODE HERE ###  (â‰ˆ 2 lines of code)\n",
    "      if probas[0,i] > 0.5:\n",
    "        p[0,i] = 1\n",
    "      else:\n",
    "        p[0,i] = 0\n",
    "\n",
    "      ### END CODE HERE ###\n",
    "\n",
    "    acc = np.sum((p == y)/m)        \n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1kPtlhwMGBiIoqhMNNRPyL3dfcyXbRib2",
     "timestamp": 1665827399846
    },
    {
     "file_id": "1fEsC8gibd-d6xC5KkfLCE6g_m9lVRKeq",
     "timestamp": 1665138833048
    }
   ]
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
